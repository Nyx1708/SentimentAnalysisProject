---
title: "SentimentAnalysisProject_Sobusa_Tamonan_Delgado.Rmd"
author: "Nexon Sobusa"
date: "2024-12-14"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("lubridate")
#install.packages("tidytext")

library(dplyr)
library(stringr)
library(ggplot2)
library(lubridate)
library(tidytext)
```

```{r}
tweets_data <- read.csv("D:/SentimentAnalysisProject/tweetsDF.csv")
```

```{r}
# Remove duplicates
tweets_data <- tweets_data %>%
  distinct()
```

```{r}
missing_values_count <- colSums(is.na(tweets_data))
```

```{r}
# Clean text column
tweets_data$text <- tweets_data$text %>%
  str_replace_all("http\\S+|www\\.\\S+", "") %>% # Remove URLs
  str_replace_all("[^[:alnum:][:space:]]", "") %>% # Remove special characters
  str_squish() # Remove extra whitespaces
```

```{r}
print(head(tweets_data))

```

```{r}
print(missing_values_count)
```

```{r}
# Trend Analysis
```

```{r}
tweets_data$created_at <- ymd_hms(tweets_data$created)
```

```{r}
daily_tweet_counts <- tweets_data %>%
  mutate(tweet_date = as_date(created_at)) %>%
  group_by(tweet_date) %>%
  summarise(tweet_count = n())
```

```{r}
ggplot(daily_tweet_counts, aes(x = tweet_date, y = tweet_count)) +
  geom_line(color = "black", linewidth = 1) +
  labs(
    title = "Daily Tweet Trend",
    x = "Date",
    y = "Number of Tweets"
  ) +
  theme_minimal()
```

```{r}
# Sentimental Analysis
```

```{r}
bing_sentiment_lexicon <- get_sentiments("bing")
```

```{r}
tweet_words <- tweets_data %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = join_by(word))
```

```{r}
sentiment_results <- tweet_words %>%
  inner_join(bing_sentiment_lexicon, by = "word") %>%
  count(sentiment) %>%
  mutate(percent = n / sum(n) * 100)
```

```{r}
ggplot(sentiment_results, aes(x = sentiment, y = percent, fill = sentiment)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Sentiment Analysis of Tweets",
    x = "Sentiment",
    y = "Percentage of Words"
  ) +
  theme_minimal()
```